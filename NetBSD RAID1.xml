<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD Simplified DocBook XML V1.1//EN"
"http://www.oasis-open.org/docbook/xml/simple/1.1/sdocbook.dtd">
<article>
  <title>NetBSD 4.0 Installation Quick and Clean - Software RAID 1
  Setup</title>

  <section>
    <title></title>

    <para></para>

    <section>
      <title>RAID 1 Setup if you have two identical disks (optional)</title>

      <para>WARNING : Be extra carefull here, as one little mistake can
      destroy your newly installed system. I hope this kind of stuff will make
      it at some point into sysinst, but nothing is available right now, as
      far as I know...</para>

      <para>We need to make sure RAID support is enabled in the kernel, which
      is the case for the GENERIC kernel. If you have already built a custom
      kernel for your environment, the kernel configuration must have the
      following options:</para>

      <programlisting>pseudo-device   raid            8       # RAIDframe disk driver
options         RAID_AUTOCONFIG         # auto-configuration of RAID components</programlisting>

      <para>The RAID support must be detected by the NetBSD kernel, which can
      be checked by looking at the output of the dmesg(8) command.</para>

      <programlisting># dmesg | grep -i raid
Kernelized RAIDframe activated</programlisting>

      <para>We assume here that you already installed NetBSD as usual on
      wd0.</para>

      <para>To improve disk performance, and only if you have a power
      redundancy or an UPS, you can enable read and write caches on the drives
      if it s not already done. To check that and change it if needed, use the
      dkctl utility :</para>

      <programlisting># dkctl wd0 getcache
/dev/rwd0d: read cache enabled
/dev/rwd0d: read cache enable is not changeable
/dev/rwd0d: write cache enable is changeable
/dev/rwd0d: cache parameters are not savable
# dkctl wd0 setcache rw
# dkctl wd0 getcache
/dev/rwd0d: read cache enabled
/dev/rwd0d: write-back cache enabled
/dev/rwd0d: read cache enable is not changeable
/dev/rwd0d: write cache enable is changeable
/dev/rwd0d: cache parameters are not savable</programlisting>

      <para>Now lets configure wd1 properly to mirror the data.</para>

      <programlisting>#dd if=/dev/zero of=/dev/rwd1d bs=8k count=1
1+0 records in
1+0 records out
8192 bytes transferred in 0.003 secs (2730666 bytes/sec)</programlisting>

      <para>Now we need to properly setup wd1 for RAID, and activating the
      correct partition to be able to boot on it. Don't bother about wd0 for
      now, it will join the RAID array later on.</para>

      <programlisting># fdisk -0ua /dev/rwd1d
fdisk: primary partition table invalid, no magic in sector 0
Disk: /dev/rwd1d
NetBSD disklabel disk geometry:
cylinders: 19386, heads: 16, sectors/track: 63 (1008 sectors/cylinder)
total sectors: 19541088

BIOS disk geometry:
cylinders: 1023, heads: 255, sectors/track: 63 (16065 sectors/cylinder)
total sectors: 19541088

Do you want to change our idea of what BIOS thinks? [n]

Partition 0:
&lt;UNUSED&gt;
The data for partition 0 is:
&lt;UNUSED&gt;
sysid: [0..255 default: 169]
start: [0..1216cyl default: 63, 0cyl, 0MB]
size: [0..1216cyl default: 19541025, 1216cyl, 9542MB]
bootmenu: []
Do you want to change the active partition? [n] y
Choosing 4 will make no partition active.
active partition: [0..4 default: 0] 0
Are you happy with this choice? [n] y

We haven't written the MBR back to disk yet.  This is your last chance.
Partition table:
0: NetBSD (sysid 169)
    start 63, size 19541025 (9542 MB, Cyls 0-1216/96/1), Active
        PBR is not bootable: All bytes are identical (0x00)
1: &lt;UNUSED&gt;
2: &lt;UNUSED&gt;
3: &lt;UNUSED&gt;
Bootselector disabled.
Should we write new partition table? [n] y

# disklabel -r -e -I wd1
type: unknown
disk: Disk1
label:
flags:
bytes/sector: 512
sectors/track: 63
tracks/cylinder: 16
sectors/cylinder: 1008
cylinders: 19386
total sectors: 19541088
[...snip...]
16 partitions:
#        size    offset     fstype [fsize bsize cpg/sgs]
 a:  19541025        63       RAID                     # (Cyl.      0*-19385)
 c:  19541025        63     unused      0     0        # (Cyl.      0*-19385)
 d:  19541088         0     unused      0     0        # (Cyl.      0 -19385)</programlisting>

      <para>Next we create the configuration file for the RAID set / volume.
      Traditionally, RAIDframe configuration files belong in /etc and would be
      read and initialized at boot time, however, because we are creating a
      bootable RAID volume, the configuration data will actually be written
      into the RAID volume using the "auto-configure" feature. Therefore,
      files are needed only during the initial setup and should not reside in
      /etc.</para>

      <programlisting># vi /var/tmp/raid0.conf
START array
1 2 0

START disks
absent
/dev/wd1a

START layout
128 1 1 1

START queue
fifo 100</programlisting>

      <para>Note that "absent" is a special disk name for a non existing disk.
      We will put wd0 there later on.</para>

      <para>Next we configure the RAID device and initialize the serial number
      to something unique. In this example we use a "YYYYMMDDRevision" scheme.
      The format you choose is entirely at your discretion, however the scheme
      you choose should ensure that no two RAID sets use the same serial
      number at the same time.</para>

      <para>After that we initialize the RAID set for the first time, safely
      ignoring the errors regarding the bogus component.</para>

      <programlisting># raidctl -v -C /var/tmp/raid0.conf raid0
Ignoring missing component at column 0
Hosed component: /dev/wd1a
Hosed component: /dev/wd1a
raid0: Component absent being configured at col: 0
         Column: 0 Num Columns: 0
         Version: 0 Serial Number: 0 Mod Counter: 0
         Clean: No Status: 0
Number of columns do not match for: absent
absent is not clean!
raid0: Ignoring /dev/wd1a
raid0: There were fatal errors
raid0: Fatal errors being ignored.
raid0: RAID Level 1
raid0: Components: component0[**FAILED**] /dev/wd1a
raid0: Total Sectors: 19540864 (9541 MB)
# raidctl -v -I 2004082401 raid0
# raidctl -v -i raid0
Initiating re-write of parity
raid0: Error re-writing parity!
Parity Re-write status:

# raidctl -v -s raid0
Components:
           component0: failed
           /dev/wd1a: optimal
No spares.
component0 status is: failed.  Skipping label.
Component label for /dev/wd1a:
   Row: 0, Column: 1, Num Rows: 1, Num Columns: 2
   Version: 2, Serial Number: 2004082401, Mod Counter: 7
   Clean: No, Status: 0
   sectPerSU: 128, SUsPerPU: 1, SUsPerRU: 1
   Queue size: 100, blocksize: 512, numBlocks: 19540864
   RAID Level: 1
   Autoconfig: No
   Root partition: No
   Last configured as: raid0
Parity status: DIRTY
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.</programlisting>

      <para>Careful : The root filesystem must begin at sector 0 of the RAID
      device. Else, the primary boot loader will be unable to find the
      secondary boot loader.</para>

      <para>The RAID device is now configured and available. The RAID device
      is a pseudo disk-device. It will be created with a default disk label.
      Now we need to setup the correct filesystems on different slices as we
      decided earlier for the usual installation. Once again you need to edit
      the disklabel:</para>

      <programlisting># disklabel -r -e -I raid0</programlisting>

      <para>Once the slices have been specified, format the newly created
      partitions as a 4.2BSD FFSv2 File System:</para>

      <programlisting># newfs -O 2 /dev/rraid0a &amp;&amp; fsck -fy /dev/rraid0a</programlisting>

      <para>Do that for each slice you created on your raid
      pseudo-drive.</para>

      <para>Now we need to setup space for kernel dumps. Unfortunately when a
      dumps has to happen, process scheduling is stopped, which forbids us to
      use raid0 swap space for it. However, nothing stops us from defining a
      dump area which overlaps with raid0b. The trick here is to calculate the
      correct start offset for our crash dump area. This is dangerous and it
      is possible to destroy valuable data if we make a mistake in these
      calculations! Data corruption will happen when the kernel writes its
      memory dump over a normal filesystem. So we must be extra careful
      here.</para>

      <para>I will use a simple configuration example here, as found in the
      NetBSD guide. First we need to take a look at the disklabel for swap
      (raid0b) and the real physical disk (wd1).</para>

      <programlisting># disklabel raid0
 8 partitions:
 #        size    offset     fstype  [fsize bsize cpg/sgs]
  a:  19015680         0     4.2BSD   1024  8192    64
  b:    525184  19015680     swap
  d:  19540864         0     unused      0     0     0

# disklabel wd1
 8 partitions:
 #        size    offset     fstype  [fsize bsize cpg/sgs]
  a:  19541025        63     RAID
  c:  19541025        63     unused      0     0
  d:  19541088         0     unused      0     0</programlisting>

      <para>Each component of a RAID set has a 64 block reserved area (see
      RF_PROTECTED_SECTORS in &lt;dev/raidframe/raidframevar.h&gt;) in the
      beginning of the component to store the internal RAID structures.</para>

      <programlisting># dc
 63              # offset of wd1a
 64              # RF_PROTECTED_SECTORS
 +
 19015680        # offset of raid0b
 +p
 19015807        # offset of swap within wd1
 q</programlisting>

      <para>We know now that real offset of the still-nonexisting wd1b is
      19015807 and size is 525184. Next we need to add wd1b to wd1's
      disklabel.</para>

      <programlisting># disklabel wd1 &gt; disklabel.wd1
# vi disklabel.wd1

 8 partitions:
 #        size    offset     fstype  [fsize bsize cpg/sgs]
  a:  19541025        63       RAID
  b:    525184  19015807       swap
  c:  19541025        63     unused      0     0
  d:  19541088         0     unused      0     0 </programlisting>

      <para>Next we install the new disklabel.</para>

      <programlisting># disklabel -R -r wd1 disklabel.wd1</programlisting>

      <para>The new RAID filesystems are now ready for use. We mount them
      under /mnt and copy all files from the old system. This can be done
      using dump(8) or pax(1).</para>

      <programlisting># mount /dev/raid0a /mnt
# df -h /mnt
Filesystem    Size     Used     Avail Capacity  Mounted on
/dev/raid0a   9.0G     2.0K      8.6G     0%    /mnt
# cd /; pax -v -X -rw -pe / /mnt</programlisting>

      <para>The NetBSD install now exists on the RAID filesystem. We need to
      fix the mount-points in the new copy of /etc/fstab or the system will
      not come up correctly. Replace instances of wd0 with raid0.</para>

      <para>Note that the kernel crash dumps must not be saved on a RAID
      device but on a real physical disk (wd0b). This dump area was created in
      the previous chapter on the second disk (wd1b) but we will make wd0 an
      identical copy of wd1 later so wd0b and wd1b will have the same size and
      offset. If wd0 fails and is removed from the server wd1 becomes wd0
      after reboot and crash dumps will still work as we are using wd0b in
      /etc/fstab. The only fault in this configuration is when the original,
      failed wd0 is replaces by a new drive and we haven't initialized it yet
      with fdisk and disklabel. In this short period of time we can not make
      crash dumps in case of kernel panic. Note how the dump device has the
      “dp” keyword on the 4th field.</para>

      <programlisting># vi /mnt/etc/fstab

/dev/raid0a  /  ffs  rw  1  1
/dev/raid0b  none  swap  sw  0  0
/dev/wd0b  none  swap  dp  0  0
kernfs    /kern  kernfs  rw
procfs    /proc  procfs  rw</programlisting>

      <para>The swap should be unconfigured upon shutdown to avoid parity
      errors on the RAID device. This can be done with a simple, one-line
      setting in /etc/rc.conf.</para>

      <programlisting># vi /mnt/etc/rc.conf
swapoff=YES</programlisting>

      <para>Next the boot loader must be installed on wd1. Failure to install
      the loader on wd1 will render the system un-bootable if wd0 fails making
      the RAID-1 pointless.</para>

      <para>Although it may seem logical to install the 1st stage boot block
      into /dev/rwd1{c,d} (which is historically correct with NetBSD 1.6.x
      installboot(8) , this is no longer the case. If you make this mistake,
      the boot sector will become irrecoverably damaged and you will need to
      start the process over again.</para>

      <para>Install the boot loader into /dev/rwd1a :</para>

      <programlisting># /usr/sbin/installboot -o timeout=30 -v /dev/rwd1a /usr/mdec/bootxx_ffsv2</programlisting>

      <para>Hint : using different timeout values can help you to determine
      from which physical disk the system is actually booting.</para>

      <para>Finally the RAID set must be made auto-configurable and the system
      should be rebooted. After the reboot everything is mounted from the RAID
      devices.</para>

      <programlisting># raidctl -v -A root raid0
raid0: New autoconfig value is: 1
raid0: New rootpartition value is: 1
raid0: Autoconfigure: Yes
raid0: Root: Yes
# raidctl -v -s raid0
[...snip...]
   Autoconfig: Yes
   Root partition: Yes
   Last configured as: raid0
[...snip...]
# shutdown -r now</programlisting>

      <para>WARNING : Always use shutdown(8) when shutting down. Never simply
      use reboot(8). reboot(8) will not properly run shutdown RC scripts and
      will not safely disable swap. This will cause dirty parity at every
      reboot.</para>

      <para>To test that your raid drive boots fine, change your BIOS settings
      to boot wd1 before wd0.</para>

      <para>The system should boot now and all filesystems should be on the
      RAID devices. The RAID will be functional with a single component,
      however the set is not fully functional because one drive is
      "absent".</para>

      <programlisting># egrep -i "raid|root" /var/run/dmesg.boot
raid0: RAID Level 1
raid0: Components: component0[**FAILED**] /dev/wd1a
raid0: Total Sectors: 19540864 (9541 MB)
boot device: raid0
root on raid0a dumps on raid0b
root file system type: ffs

# df -h
Filesystem    Size     Used     Avail Capacity  Mounted on
/dev/raid0a   8.9G     196M      8.3G     2%    /
kernfs        1.0K     1.0K        0B   100%    /kern

# swapctl -l
Device      1K-blocks     Used    Avail Capacity  Priority
/dev/raid0b    262592        0   262592     0%    0
# raidctl -s raid0
Components:
          component0: failed
           /dev/wd1a: optimal
No spares.
component0 status is: failed.  Skipping label.
Component label for /dev/wd1a:
   Row: 0, Column: 1, Num Rows: 1, Num Columns: 2
   Version: 2, Serial Number: 2004082401, Mod Counter: 65
   Clean: No, Status: 0
   sectPerSU: 128, SUsPerPU: 1, SUsPerRU: 1
   Queue size: 100, blocksize: 512, numBlocks: 19540864
   RAID Level: 1
   Autoconfig: Yes
   Root partition: Yes
   Last configured as: raid0
Parity status: DIRTY
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.</programlisting>

      <para>We will now add Disk0/wd0 as a component of the RAID. This will
      destroy the original file system structure. On i386, the MBR disklabel
      will be unaffected (remember we copied wd0's label to wd1 anyway) ,
      therefore there is no need to "zero" Disk0/wd0. However, we need to
      relabel Disk0/wd0 to have an identical NetBSD disklabel layout as
      Disk1/wd1. Then we add Disk0/wd0 as "hot-spare" to the RAID set and
      initiate the parity reconstruction for all RAID devices, effectively
      bringing Disk0/wd0 into the RAID-1 set and "synching up" both
      disks.</para>

      <programlisting># disklabel -r wd1 &gt; /tmp/disklabel.wd1
# disklabel -R -r wd0 /tmp/disklabel.wd1</programlisting>

      <para>As a last-minute sanity check, you might want to use diff(1) to
      ensure that the disklabels of Disk0/wd0 match Disk1/wd1. You should also
      backup these files for reference in the event of an emergency.</para>

      <programlisting># disklabel -r wd0 &gt; /tmp/disklabel.wd0
# disklabel -r wd1 &gt; /tmp/disklabel.wd1
# diff /tmp/disklabel.wd0 /tmp/disklabel.wd1
# fdisk /dev/rwd0 &gt; /tmp/fdisk.wd0
# fdisk /dev/rwd1 &gt; /tmp/fdisk.wd1
# diff /tmp/fdisk.wd0 /tmp/fdisk.wd1
# mkdir /root/RFbackup
# cp -v -p /tmp/disklabel.* /tmp/fdisk.* /root/RFbackup</programlisting>

      <para>Once you are certain, add Disk0/wd0 as a spare component, and
      start reconstruction:</para>

      <programlisting># raidctl -v -a /dev/wd0a raid0
/netbsd: Warning: truncating spare disk /dev/wd0a to 241254528 blocks
# raidctl -v -s raid0
Components:
          component0: failed
           /dev/wd1a: optimal
Spares:
           /dev/wd0a: spare
[...snip...]
# raidctl -F component0 raid0
RECON: initiating reconstruction on col 0 -&gt; spare at col 2</programlisting>

      <para>Depending on the speed of your hardware, the reconstruction time
      will vary. You may wish to watch it on another terminal:</para>

      <programlisting># raidctl -S raid0
Reconstruction is 0% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.
  17% |******                                 | ETA: 03:08 -</programlisting>

      <para>After reconstruction, both disks should be “optimal”.</para>

      <programlisting># tail -f /var/log/messages
raid0: Reconstruction of disk at col 0 completed
raid0: Recon time was 1290.625033 seconds, accumulated XOR time was 0 us (0.000000)
raid0:  (start time 1093407069 sec 145393 usec, end time 1093408359 sec 770426 usec)
raid0: Total head-sep stall count was 0
raid0: 305318 recon event waits, 1 recon delays
raid0: 1093407069060000 max exec ticks

# raidctl -v -s raid0
Components:
           component0: spared
           /dev/wd1a: optimal
Spares:
     /dev/wd0a: used_spare
     [...snip...]</programlisting>

      <para>When the reconstruction is finished we need to install the boot
      loader on the Disk0/wd0. On i386, install the boot loader into
      /dev/rwd0a:</para>

      <programlisting># /usr/sbin/installboot -o timeout=2 -v /dev/rwd0a /usr/mdec/bootxx_ffsv2
File system:         /dev/rwd1a
File system type:    raw (blocksize 8192, needswap 1)
Primary bootstrap:   /usr/mdec/bootxx_ffsv2
Preserving 51 (0x33) bytes of the BPB</programlisting>

      <para>And finally, reboot the machine one last time before
      proceeding.</para>

      <programlisting># shutdown -r now</programlisting>

      <para>This is required to migrate Disk0/wd0 from status "used_spare" as
      "Component0" to state "optimal".</para>

      <para>At this point, edit you BIOS setting to ensure that both disks are
      bootable and that your system runs fine regardless of which disk is
      booted. Note that you can determine the disk the machine booted on
      because of the timeout value.</para>

      <para>At each boot, the following should appear in the NetBSD kernel
      dmesg(8) :</para>

      <programlisting>raid0: RAID Level 1
raid0: Components: /dev/wd0a /dev/wd1a
raid0: Total Sectors: 19540864 (9541 MB)
boot device: raid0
root on raid0a dumps on raid0b
root file system type: ffs</programlisting>

      <para>Once you are certain that both disks are bootable, verify the RAID
      parity is clean after each reboot:</para>

      <programlisting># raidctl -v -s raid0
Components:
          /dev/wd0a: optimal
          /dev/wd1a: optimal
No spares.
[...snip...]
Parity status: clean
Reconstruction is 100% complete.
Parity Re-write is 100% complete.
Copyback is 100% complete.</programlisting>

      <para>The last thing to do now is to test the kernel crash dumps so that
      they work correctly and do not overwrite any important filesystems (like
      the raid0e filesystem).</para>

      <para>Press Ctrl+Alt+Esc to test the kernel crash dump. This will invoke
      the kernel debugger. Type sync or reboot 0x104 and press Enter. This
      will save the current kernel memory to the dump area (wd0b) for further
      analysis. Most likely the offset and/or size of wd0b is wrong if the
      system will not come up correctly after reboot (unable to mount /home,
      corrupted super-blocks, etc). It is very important to test this now, not
      when we have lots of valuable files in /home. One more time: take a
      backup of all your files before following these instructions!</para>

      <para>Anyway, always remember to backup regularly your data.</para>
    </section>
  </section>
</article>